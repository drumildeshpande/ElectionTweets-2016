{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "import json\n",
    "import nltk\n",
    "from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs\n",
    "from nltk.classify.api import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "import urllib3\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(train_X,train_Y,test_X,test_Y):\n",
    "    top_words = 50000\n",
    "    max_review_length = 250\n",
    "    train_X = sequence.pad_sequences(train_X, maxlen=max_review_length)\n",
    "    test_X = sequence.pad_sequences(test_X, maxlen=max_review_length)\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(train_X, train_Y, validation_data=(test_X, test_Y), nb_epoch=3, batch_size=64)\n",
    "    \n",
    "    scores = model.evaluate(test_X, test_Y, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(per_train,f1name,f2name):\n",
    "    data1 = pd.read_csv(f1name)\n",
    "    data2 = pd.read_csv(f2name)\n",
    "    size1 = (len(data1.index)-1)*per_train\n",
    "    size1/=100\n",
    "    size2 = (len(data2.index)-1)*per_train\n",
    "    size2/=100\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for i in range(0,size1):\n",
    "        temp = data1.iloc[i,3].split()\n",
    "        train_X.append(temp)\n",
    "        train_Y.append(data1.iloc[i,4])\n",
    "    \n",
    "    for i in range(size1,len(data1.index)):\n",
    "        temp = data1.iloc[i,3].split()\n",
    "        test_X.append(temp)\n",
    "        test_Y.append(data1.iloc[i,4])\n",
    "        \n",
    "    for i in range(0,size2):\n",
    "        temp = data2.iloc[i,3].split()\n",
    "        train_X.append(temp)\n",
    "        train_Y.append(data2.iloc[i,4])\n",
    "        \n",
    "    for i in range(size2,len(data2.index)):\n",
    "        temp = data2.iloc[i,3].split()\n",
    "        test_X.append(temp)\n",
    "        test_Y.append(data2.iloc[i,4])\n",
    "        \n",
    "    #print train_X\n",
    "    #print train_Y\n",
    "    \n",
    "    trainX = np.array(train_X)\n",
    "    trainY = np.array(train_Y)\n",
    "    testX = np.array(test_X)\n",
    "    testY = np.array(test_Y)\n",
    "    \n",
    "    rnn(trainX,trainY,testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 250, 32)       1600000     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           53200       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             101         lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1653301\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8282 samples, validate on 19328 samples\n",
      "Epoch 1/3\n",
      "8282/8282 [==============================] - 162s - loss: 0.6039 - acc: 0.6659 - val_loss: 0.6238 - val_acc: 0.6279\n",
      "Epoch 2/3\n",
      "8282/8282 [==============================] - 161s - loss: 0.3244 - acc: 0.8651 - val_loss: 0.3838 - val_acc: 0.8140\n",
      "Epoch 3/3\n",
      "8282/8282 [==============================] - 161s - loss: 0.3103 - acc: 0.8643 - val_loss: 0.3931 - val_acc: 0.8072\n",
      "Accuracy: 80.72%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    per_train = 30\n",
    "    f1name = \"Clinton_nn_data.csv\"\n",
    "    f2name = \"Trump_nn_data.csv\"\n",
    "    while(per_train<=30):\n",
    "        load_data(per_train,f1name,f2name)\n",
    "        per_train+=10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
